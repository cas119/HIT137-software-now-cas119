{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a5a74c2-caaf-4b43-9f69-70adfcd11d00",
   "metadata": {},
   "source": [
    "# Task 1: Extract the text from multiple CSV files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ebb1b0",
   "metadata": {},
   "source": [
    "Name : A K M SHAFIUR RAHMAN\n",
    "ID   : s372618\n",
    "Github Link : https://github.com/cas119/HIT137-software-now-cas119"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9eca97c",
   "metadata": {},
   "source": [
    "# Task 3.1: Count the Top 30 most common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60ad6b1b-bbb2-41da-a6f5-1f31125959a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>and</td>\n",
       "      <td>1858141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the</td>\n",
       "      <td>1623552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>to</td>\n",
       "      <td>1555270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>of</td>\n",
       "      <td>1530541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>was</td>\n",
       "      <td>1464862</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Word    Count\n",
       "0  and  1858141\n",
       "1  the  1623552\n",
       "2   to  1555270\n",
       "3   of  1530541\n",
       "4  was  1464862"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import csv\n",
    "\n",
    "class WordAnalyzer:\n",
    "    def __init__(self, file_path, chunk_size=1024*1024, word_number=30):\n",
    "        self.file_path = file_path\n",
    "        self.chunk_size = chunk_size\n",
    "        self.word_number = word_number\n",
    "\n",
    "    def count_words(self):\n",
    "        \"\"\"Count the occurrences of words in the file.\"\"\"\n",
    "        word_counter = Counter()\n",
    "        \n",
    "        with open(self.file_path, 'r', encoding='utf-8') as infile:\n",
    "            while True:\n",
    "                chunk = infile.read(self.chunk_size)\n",
    "                if not chunk:\n",
    "                    break\n",
    "                \n",
    "                # Process chunk\n",
    "                words = chunk.split()\n",
    "                word_counter.update(words)\n",
    "        \n",
    "        return word_counter.most_common(self.word_number)\n",
    "\n",
    "    @staticmethod\n",
    "    def save_to_csv(top_words, output_csv_path):\n",
    "        \"\"\"Save the top words and their counts to a CSV file.\"\"\"\n",
    "        with open(output_csv_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow(['Word', 'Count'])\n",
    "            writer.writerows(top_words)\n",
    "\n",
    "    def process(self, output_csv_path):\n",
    "        \"\"\"Run the word count and save the results to a CSV file.\"\"\"\n",
    "        top_words = self.count_words()\n",
    "        self.save_to_csv(top_words, output_csv_path)\n",
    "\n",
    "\n",
    "file_path = './output/combined_texts.txt'\n",
    "output_csv_path = './output/top_30_words.csv'\n",
    "\n",
    "analyzer = WordAnalyzer(file_path)\n",
    "analyzer.process(output_csv_path)\n",
    "pd.read_csv('./output/top_30_words.csv').head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5458e430",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d67f9cc2-e8cf-4df7-8822-ce584b9bf085",
   "metadata": {},
   "source": [
    "# Task 3.2: Use AutoTokenizer from transformers and Count 30 most common tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a0958aa-f341-490b-b954-af86af056e4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Token</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the</td>\n",
       "      <td>2128027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>and</td>\n",
       "      <td>1865824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>to</td>\n",
       "      <td>1612513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>of</td>\n",
       "      <td>1568772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>was</td>\n",
       "      <td>1466187</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Token    Count\n",
       "0   the  2128027\n",
       "1   and  1865824\n",
       "2    to  1612513\n",
       "3    of  1568772\n",
       "4   was  1466187"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from collections import Counter\n",
    "import csv\n",
    "\n",
    "class TokenAnalyzer:\n",
    "    def __init__(self, file_path, tokenizer_name=\"bert-base-uncased\", chunk_size=512, token_number=30):\n",
    "        self.file_path = file_path\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, clean_up_tokenization_spaces=True)\n",
    "        self.chunk_size = chunk_size\n",
    "        self.token_number = token_number\n",
    "\n",
    "    def count_tokens(self):\n",
    "        \"\"\"Count the occurrences of tokens in the file.\"\"\"\n",
    "        token_counts = Counter()\n",
    "        \n",
    "        with open(self.file_path, 'r', encoding='utf-8') as infile:\n",
    "            while True:\n",
    "                chunk = infile.read(self.chunk_size)\n",
    "                if not chunk:\n",
    "                    break\n",
    "                \n",
    "                # Process chunk\n",
    "                tokens = self.tokenizer.tokenize(chunk)\n",
    "                token_counts.update(tokens)\n",
    "        \n",
    "        return token_counts.most_common(self.token_number)\n",
    "\n",
    "    @staticmethod\n",
    "    def save_to_csv(top_tokens, output_csv_path):\n",
    "        \"\"\"Save the top tokens and their counts to a CSV file.\"\"\"\n",
    "        with open(output_csv_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow(['Token', 'Count'])\n",
    "            writer.writerows(top_tokens)\n",
    "\n",
    "    def process(self, output_csv_path):\n",
    "        \"\"\"Run the token count and save the results to a CSV file.\"\"\"\n",
    "        top_tokens = self.count_tokens()\n",
    "        self.save_to_csv(top_tokens, output_csv_path)\n",
    "\n",
    "\n",
    "file_path = './output/combined_texts.txt'\n",
    "output_csv_path = './output/top_30_tokens.csv'\n",
    "\n",
    "analyzer = TokenAnalyzer(file_path)\n",
    "analyzer.process(output_csv_path)\n",
    "pd.read_csv('./output/top_30_tokens.csv').head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cc4b58-f0c5-4a91-8bd5-23174defb657",
   "metadata": {},
   "source": [
    "# Task 4: Named-Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bd3683-d422-4c1b-90df-31dad32388a5",
   "metadata": {},
   "source": [
    "***SciSpacy & \n",
    "en_ner_bc5cdr_md***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fea1fa-c533-4f5b-a2b7-3cab6d24642b",
   "metadata": {},
   "source": [
    "***BioBert***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50dfdb1d-3c0d-40b3-94fe-78abb116430b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "from collections import Counter, defaultdict\n",
    "import os\n",
    "\n",
    "class EntityAnalyzer:\n",
    "    def __init__(self, file_path, chunk_size=1000000):\n",
    "        self.file_path = file_path\n",
    "        self.chunk_size = chunk_size\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # Load spaCy model\n",
    "        self.spacy_model_name = \"en_ner_bc5cdr_md\"\n",
    "        self.nlp = spacy.load(self.spacy_model_name)\n",
    "        \n",
    "        # Load BioBERT model and tokenizer for disease NER\n",
    "        self.biobert_model_disease = AutoModelForTokenClassification.from_pretrained(\"ugaray96/biobert_ncbi_disease_ner\").to(self.device)\n",
    "        self.biobert_tokenizer_disease = AutoTokenizer.from_pretrained(\"ugaray96/biobert_ncbi_disease_ner\")\n",
    "        \n",
    "        # Load ClinicalNER-PT model and tokenizer for drug NER\n",
    "        self.drug_model = AutoModelForTokenClassification.from_pretrained(\"pucpr/clinicalnerpt-chemical\").to(self.device)\n",
    "        self.drug_tokenizer = AutoTokenizer.from_pretrained(\"pucpr/clinicalnerpt-chemical\")\n",
    "        \n",
    "        # Initialize NER pipelines for both disease and drug models\n",
    "        self.biobert_ner_disease = pipeline(\"ner\", model=self.biobert_model_disease, tokenizer=self.biobert_tokenizer_disease, device=0 if torch.cuda.is_available() else -1)\n",
    "        self.drug_ner = pipeline(\"ner\", model=self.drug_model, tokenizer=self.drug_tokenizer, device=0 if torch.cuda.is_available() else -1)\n",
    "        \n",
    "        self.spacy_diseases = {}\n",
    "        self.spacy_drugs = {}\n",
    "        self.biobert_diseases = {}\n",
    "        self.biobert_drugs = {}\n",
    "\n",
    "    def _process_text_with_spacy(self, text_chunk):\n",
    "        \"\"\"Process a text chunk with spaCy to extract diseases and drugs.\"\"\"\n",
    "        doc = self.nlp(text_chunk)\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ == \"DISEASE\":\n",
    "                self.spacy_diseases[ent.text] = self.spacy_diseases.get(ent.text, 0) + 1\n",
    "            elif ent.label_ == \"CHEMICAL\":\n",
    "                self.spacy_drugs[ent.text] = self.spacy_drugs.get(ent.text, 0) + 1\n",
    "\n",
    "    def _process_text_with_biobert(self, text_chunk):\n",
    "        \"\"\"Process a text chunk with BioBERT to extract diseases and drugs.\"\"\"\n",
    "        biobert_entities_disease = self.biobert_ner_disease(text_chunk)\n",
    "        for ent in biobert_entities_disease:\n",
    "            if ent['entity'] == \"Disease\":\n",
    "                self.biobert_diseases[ent['word']] = self.biobert_diseases.get(ent['word'], 0) + 1\n",
    "\n",
    "        drug_entities = self.drug_ner(text_chunk)\n",
    "        for ent in drug_entities:\n",
    "            if ent['entity'] in [\"B-ChemicalDrugs\", \"I-ChemicalDrugs\"]:\n",
    "                self.biobert_drugs[ent['word']] = self.biobert_drugs.get(ent['word'], 0) + 1\n",
    "\n",
    "    def process_text_file(self):\n",
    "        \"\"\"Read the text file in chunks and process it with both spaCy and BioBERT models.\"\"\"\n",
    "        with open(self.file_path, 'r', encoding='utf-8') as infile:\n",
    "            while True:\n",
    "                chunk = infile.read(self.chunk_size)\n",
    "                if not chunk:\n",
    "                    break\n",
    "                chunk = chunk.strip()\n",
    "                if chunk:\n",
    "                    self._process_text_with_spacy(chunk)\n",
    "                    self._process_text_with_biobert(chunk)\n",
    "                break #Used To Reduce Time....Just Comment this line#\n",
    "    def get_results(self):\n",
    "        \"\"\"Get total counts and most common entities from both models.\"\"\"\n",
    "        total_spacy_diseases = len(self.spacy_diseases)\n",
    "        total_spacy_drugs = len(self.spacy_drugs)\n",
    "        most_common_spacy_diseases = Counter(self.spacy_diseases).most_common(10)\n",
    "        most_common_spacy_drugs = Counter(self.spacy_drugs).most_common(10)\n",
    "        \n",
    "        total_biobert_diseases = len(self.biobert_diseases)\n",
    "        total_biobert_drugs = len(self.biobert_drugs)\n",
    "        most_common_biobert_diseases = Counter(self.biobert_diseases).most_common(10)\n",
    "        most_common_biobert_drugs = Counter(self.biobert_drugs).most_common(10)\n",
    "        \n",
    "        return {\n",
    "            \"total_spacy_diseases\": total_spacy_diseases,\n",
    "            \"most_common_spacy_diseases\": most_common_spacy_diseases,\n",
    "            \"total_spacy_drugs\": total_spacy_drugs,\n",
    "            \"most_common_spacy_drugs\": most_common_spacy_drugs,\n",
    "            \"total_biobert_diseases\": total_biobert_diseases,\n",
    "            \"most_common_biobert_diseases\": most_common_biobert_diseases,\n",
    "            \"total_biobert_drugs\": total_biobert_drugs,\n",
    "            \"most_common_biobert_drugs\": most_common_biobert_drugs\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def save_results_to_files(results, results_directory):\n",
    "        \"\"\"Save the results to text files in the specified directory.\"\"\"\n",
    "        # Create subfolders for the results\n",
    "        spacy_folder = os.path.join(results_directory, \"SciSpacy\", \"en_ner_bc5cdr_md\")\n",
    "        biobert_folder = os.path.join(results_directory, \"BioBERT\")\n",
    "        os.makedirs(spacy_folder, exist_ok=True)\n",
    "        os.makedirs(biobert_folder, exist_ok=True)\n",
    "        \n",
    "        # Save spaCy results\n",
    "        with open(os.path.join(spacy_folder, 'total_entities.txt'), 'w', encoding='utf-8') as f:\n",
    "            f.write(f\"Total diseases detected (spaCy): {results['total_spacy_diseases']}\\n\")\n",
    "            f.write(f\"Total drugs detected (spaCy): {results['total_spacy_drugs']}\\n\")\n",
    "        \n",
    "        with open(os.path.join(spacy_folder, 'most_common_diseases.txt'), 'w', encoding='utf-8') as f:\n",
    "            for disease, count in results['most_common_spacy_diseases']:\n",
    "                f.write(f\"{disease}: {count}\\n\")\n",
    "        \n",
    "        with open(os.path.join(spacy_folder, 'most_common_drugs.txt'), 'w', encoding='utf-8') as f:\n",
    "            for drug, count in results['most_common_spacy_drugs']:\n",
    "                f.write(f\"{drug}: {count}\\n\")\n",
    "        \n",
    "        # Save BioBERT results\n",
    "        with open(os.path.join(biobert_folder, 'total_entities.txt'), 'w', encoding='utf-8') as f:\n",
    "            f.write(f\"Total diseases detected (BioBERT): {results['total_biobert_diseases']}\\n\")\n",
    "            f.write(f\"Total drugs detected (BioBERT): {results['total_biobert_drugs']}\\n\")\n",
    "        \n",
    "        with open(os.path.join(biobert_folder, 'most_common_diseases.txt'), 'w', encoding='utf-8') as f:\n",
    "            for disease, count in results['most_common_biobert_diseases']:\n",
    "                f.write(f\"{disease}: {count}\\n\")\n",
    "        \n",
    "        with open(os.path.join(biobert_folder, 'most_common_drugs.txt'), 'w', encoding='utf-8') as f:\n",
    "            for drug, count in results['most_common_biobert_drugs']:\n",
    "                f.write(f\"{drug}: {count}\\n\")\n",
    "\n",
    "    def compare_and_save_results(self, results, results_directory):\n",
    "        \"\"\"Compare results from spaCy and BioBERT and save the comparison to a file.\"\"\"\n",
    "        comparison_diseases = defaultdict(lambda: {'spaCy_count': 0, 'BioBERT_count': 0})\n",
    "        comparison_drugs = defaultdict(lambda: {'spaCy_count': 0, 'BioBERT_count': 0})\n",
    "        \n",
    "        # Update comparison dictionaries with results from spaCy\n",
    "        for disease, count in self.spacy_diseases.items():\n",
    "            comparison_diseases[disease]['spaCy_count'] = count\n",
    "\n",
    "        for drug, count in self.spacy_drugs.items():\n",
    "            comparison_drugs[drug]['spaCy_count'] = count\n",
    "\n",
    "        # Update comparison dictionaries with results from BioBERT\n",
    "        for disease, count in self.biobert_diseases.items():\n",
    "            comparison_diseases[disease]['BioBERT_count'] = count\n",
    "\n",
    "        for drug, count in self.biobert_drugs.items():\n",
    "            comparison_drugs[drug]['BioBERT_count'] = count\n",
    "\n",
    "        # Save comparison results\n",
    "        comparison_folder = os.path.join(results_directory, \"Comparison\")\n",
    "        os.makedirs(comparison_folder, exist_ok=True)\n",
    "        \n",
    "        with open(os.path.join(comparison_folder, 'disease_comparison.txt'), 'w', encoding='utf-8') as f:\n",
    "            f.write(f\"{'Disease':<30} {'spaCy Count':<15} {'BioBERT Count':<15}\\n\")\n",
    "            f.write(\"=\"*60 + \"\\n\")\n",
    "            for disease, counts in comparison_diseases.items():\n",
    "                f.write(f\"{disease:<30} {counts['spaCy_count']:<15} {counts['BioBERT_count']:<15}\\n\")\n",
    "        \n",
    "        with open(os.path.join(comparison_folder, 'drug_comparison.txt'), 'w', encoding='utf-8') as f:\n",
    "            f.write(f\"{'Drug':<30} {'spaCy Count':<15} {'BioBERT Count':<15}\\n\")\n",
    "            f.write(\"=\"*60 + \"\\n\")\n",
    "            for drug, counts in comparison_drugs.items():\n",
    "                f.write(f\"{drug:<30} {counts['spaCy_count']:<15} {counts['BioBERT_count']:<15}\\n\")\n",
    "\n",
    "    def run(self, results_directory):\n",
    "        \"\"\"Run the analysis and save results.\"\"\"\n",
    "        self.process_text_file()\n",
    "        results = self.get_results()\n",
    "        self.save_results_to_files(results, results_directory)\n",
    "        self.compare_and_save_results(results, results_directory)\n",
    "\n",
    "# Example usage\n",
    "file_path = './output/combined_texts.txt'\n",
    "results_directory = './output/entity_analysis_results'\n",
    "\n",
    "analyzer = EntityAnalyzer(file_path)\n",
    "analyzer.run(results_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3176299-ec69-4ed2-af3b-334181871724",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
